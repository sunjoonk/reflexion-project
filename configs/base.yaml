# Model Hyperparameters
model:
  d_model: 512
  nhead: 8
  d_ff: 2048
  num_layers: 6
  dropout: 0.1
  max_len: 100

# Vocab Sizes (Data-dependent)
data:
  src_vocab_size: 5000
  tgt_vocab_size: 6000
  pad_idx: 0

# Training Hyperparameters
train:
  epochs: 20
  batch_size: 32
  learning_rate: 0.0001
  # Adam optimizer params
  optimizer:
    betas: [0.9, 0.98]
    eps: 1.0e-9